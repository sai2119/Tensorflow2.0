{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST USING DEEP LEARNING  .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IQmrPqEZjKZf",
        "FOOYy4VjmUZX"
      ],
      "authorship_tag": "ABX9TyOFo2D7YLKgBfZxAVuEVGMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sai2119/Tensorflow2.0/blob/master/MNIST_USING_DEEP_LEARNING_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYiOD-pFEmU",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4-P5Yi_E7a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GoKAj5iPkGe",
        "colab_type": "text"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3aye26jPiSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "mnist_data, mnist_info = tfds.load(\"mnist\", with_info=True, as_supervised=True)\n",
        "mnist_train, mnist_test = mnist_data[\"train\"], mnist_data[\"test\"]\n",
        "assert isinstance(mnist_train, tf.data.Dataset)\n",
        "\n",
        "# spliting train data with 10 % to get validation data\n",
        "num_validation_sample =0.1*mnist_info.splits['train'].num_examples\n",
        "#coverting validation to int 64\n",
        "num_validation_sample=tf.cast(num_validation_sample,tf.int64)\n",
        "# same to test data\n",
        "num_test_sample = mnist_info.splits['test'].num_examples\n",
        "#coverting test to int 64\n",
        "num_test_sample=tf.cast(num_test_sample,tf.int64)\n",
        "\n",
        "\n",
        "\n",
        "# custm fuction to scale 0 to 1 \n",
        "def scale(image, label):\n",
        "  # cast to make sure that all date in float \n",
        "  image =tf.cast(image, tf.float32)\n",
        "  # As we KNow grey scale is 0 to 255 we divide the iamge by 255 to get o to 1\n",
        "  image/=255.\n",
        "  return image,label\n",
        "\n",
        "scaled_train_and_validation_data =mnist_train.map(scale)\n",
        "# dataset.map(*function*)applies a customtransformation to agiven dataset it take as input a funxtion which determines the transformation\n",
        " # scaling test value\n",
        "scaled_test_data =mnist_test.map(scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2zr_EHyUMKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shuffled data \n",
        "# helps to diorder the formats of data with same information\n",
        "BUFFER_SIZE =10000\n",
        "# buffer size is used when data is to high it says tensor flow to perform buffer size shuffles \n",
        "#buffer_size =1 no shuffing will actually happen\n",
        "# If buffer_size >=num_samples,shuffling will happen at once (uniformly)\n",
        "# If 1<Buffer_size<num_samples, we will be optimizing the computational power\n",
        "\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_sample)\n",
        "train_data =shuffled_train_and_validation_data.skip(num_validation_sample)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzv148Rp0ql3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batching\n",
        "# batchsize =1=sgd\n",
        "# batch size = # samples =(single batch) GD\n",
        "BATCH_SIZE =150\n",
        "# method dataset.batch(batch_size)\n",
        "# a method that combines the consecutive elements of a dataset into batches\n",
        "\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "# no need for validation because we only use feed forward for validation we just calculate loss\n",
        "# but model excepts validation to be in batchs \n",
        "validation_data = validation_data.batch(num_validation_sample)\n",
        "test_data =scaled_test_data.batch(num_test_sample)\n",
        "\n",
        "# iter() creates an object which can be iterated one element at atime (e.g in afor loop or while loop)\n",
        "validation_inputs, validation_targets = next(iter(validation_data))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1ZcIGMw6Z18",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6L-UsX6LiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image is in the size of 28*28 so input size is 784\n",
        "input_size =784\n",
        "\n",
        "# output is selected 10 because of ten layers \n",
        "output_size = 10\n",
        "# this hyper parameter than can be shuffled \n",
        "hidden_layer_size =500\n",
        "#the underlying assumption is that all hidden layers are of same size  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5CrDYCS-r_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                            # our data(from tfds) is such that each input is 28*28*1 \n",
        "                            # tf.keras.layers.Flatten(original shape) transforms ((flattens)a tensor into vector)\n",
        "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "                            # activation function relu, sigmoid (logistic), tanh(prabolic function), softmax\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "\n",
        "                            ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hEq60W7VOwK",
        "colab_type": "text"
      },
      "source": [
        "optimizer and the loss fuction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Sd-5bwdVNYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizer are 3 in tf2 ada,rem,ada+movmentum (adam) best is adam \n",
        "# Loss function as it is classification problem we cross entropy in tf we have 3 types\n",
        "# binary , categorical_crossentropy, sparse_categorical_crossentropy(one-hot encoding)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHfJ0VaqZq6d",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU55McQLXgGB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "b479473a-438b-4d3e-a88e-a5ca7f6c1ea2"
      },
      "source": [
        "# At the beginning of each epoch, the training loss will be set to 0\n",
        "# the algorithm will iterate over a preset number of batches,all from train_data\n",
        "# the weights and bias will be updated as many times as there are batches\n",
        "# we will get a value for the loss function indicating how the training is going\n",
        "# we will also see a training accuracy\n",
        "# at the end of the epoch the alogorithm will forward propagate the whole validation set\n",
        "# when we reach the maximum number of epochs the training will be over\n",
        "NUM_EPOCHS = 10\n",
        "model.fit(train_data,epochs=NUM_EPOCHS,validation_data=(validation_inputs,validation_targets), verbose=2)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "360/360 - 25s - loss: 0.2692 - accuracy: 0.9182 - val_loss: 0.1120 - val_accuracy: 0.9655\n",
            "Epoch 2/10\n",
            "360/360 - 25s - loss: 0.1236 - accuracy: 0.9653 - val_loss: 0.0959 - val_accuracy: 0.9735\n",
            "Epoch 3/10\n",
            "360/360 - 26s - loss: 0.0931 - accuracy: 0.9738 - val_loss: 0.0802 - val_accuracy: 0.9778\n",
            "Epoch 4/10\n",
            "360/360 - 25s - loss: 0.0793 - accuracy: 0.9781 - val_loss: 0.0765 - val_accuracy: 0.9790\n",
            "Epoch 5/10\n",
            "360/360 - 25s - loss: 0.0665 - accuracy: 0.9818 - val_loss: 0.0544 - val_accuracy: 0.9857\n",
            "Epoch 6/10\n",
            "360/360 - 25s - loss: 0.0560 - accuracy: 0.9851 - val_loss: 0.0746 - val_accuracy: 0.9813\n",
            "Epoch 7/10\n",
            "360/360 - 26s - loss: 0.0523 - accuracy: 0.9858 - val_loss: 0.0498 - val_accuracy: 0.9877\n",
            "Epoch 8/10\n",
            "360/360 - 25s - loss: 0.0509 - accuracy: 0.9861 - val_loss: 0.0431 - val_accuracy: 0.9900\n",
            "Epoch 9/10\n",
            "360/360 - 25s - loss: 0.0425 - accuracy: 0.9892 - val_loss: 0.0438 - val_accuracy: 0.9892\n",
            "Epoch 10/10\n",
            "360/360 - 25s - loss: 0.0381 - accuracy: 0.9898 - val_loss: 0.0593 - val_accuracy: 0.9862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f486b9eac18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5jI1HPhgIAw",
        "colab_type": "text"
      },
      "source": [
        "evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqZrHQPaeJuL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa1f4b13-3dd7-4e1e-c40b-562ce4e58fbe"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1080 - accuracy: 0.9752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzYFeREYgJ_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7822b4ec-400b-4784-90f0-a60b503ab0b8"
      },
      "source": [
        "# We can apply some nice formatting if we want to\n",
        "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.11. Test accuracy: 97.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQmrPqEZjKZf",
        "colab_type": "text"
      },
      "source": [
        "# Adjustments\n",
        "\n",
        "There are several main adjustments you may try.\n",
        "\n",
        "Please pay attention to the time it takes for each epoch to conclude.\n",
        "\n",
        "Using the code from the lecture as the basis, fiddle with the hyperparameters of the algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0tNgzE_kI0l",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 2\n",
        "*   activation function = relu for hidden layer \n",
        "*   activation function = soft max for output layer \n",
        "*   excution time = 11 sec\n",
        "*   loss function(trian, validation) =(0.0983,0.0939)\n",
        "*   accuracy(trian, validation) =\n",
        "*   loss function(test) =0.11\n",
        "*   accuracy(test) =96.94%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOOYy4VjmUZX",
        "colab_type": "text"
      },
      "source": [
        "1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTWVOEmamUVQ",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "\n",
        "*   batch size 200\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 2\n",
        "*   activation function = relu for hidden layer \n",
        "*   activation function = soft max for output layer \n",
        "*   excution time = 11 sec\n",
        "*   loss function(trian, validation) =(0.1188,0.1113)\n",
        "*   accuracy(trian, validation) =(0.9655,0.9673)\n",
        "*   loss function(test) =0.13\n",
        "*   accuracy(test) = 96.11%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIJRETWimUT1",
        "colab_type": "text"
      },
      "source": [
        "2. The *depth* of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViN6a8lZmUO5",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 3\n",
        "*   activation function = relu for hidden layer \n",
        "*   activation function = soft max for output layer \n",
        "*   excution time = 10s\n",
        "*   loss function(trian, validation) =0.1068,0.1068\n",
        "*   accuracy(trian, validation) =0.9680,0.9687\n",
        "*   loss function(test) =0.12\n",
        "*   accuracy(test) =96.28%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc1iI4CVmUNe",
        "colab_type": "text"
      },
      "source": [
        "3. The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt1HP2RlmUIv",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 5\n",
        "*   activation function = relu for hidden layer \n",
        "*   activation function = soft max for output layer \n",
        "*   excution time = 11s\n",
        "*   loss function(trian, validation) =0.1062,0.1067\n",
        "*   accuracy(trian, validation) =0.9678, 0.9663\n",
        "*   loss function(test) =0.12.\n",
        "*   accuracy(test) =96.35%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSbdScSomUHU",
        "colab_type": "text"
      },
      "source": [
        "# 4. Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the string 'sigmoid'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjMjkV7omUCn",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 5\n",
        "*   activation function = sigmoid for hidden layer \n",
        "*   activation function = sigmoid for output layer \n",
        "*   excution time = 11s\n",
        "*   loss function(trian, validation) =0.3883,0.3469\n",
        "*   accuracy(trian, validation) =0.9078,0.9193\n",
        "*   loss function(test) =0.36. \n",
        "\n",
        "other model\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 5\n",
        "*   activation function = softmax for hidden layer \n",
        "*   activation function = softmax for output layer \n",
        "*   excution time = 11s\n",
        "*   loss function(trian, validation) =2.3012, 2.3013\n",
        "*   accuracy(trian, validation) =0.1129, 0.1090\n",
        "*   loss function(test) =2.30. \n",
        "*   Test accuracy: 11.35%\n",
        "\n",
        "other model\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 5\n",
        "*   activation function = sigmoid for hidden layer \n",
        "*   activation function = softmax for output layer \n",
        "*   excution time = 11s\n",
        "*   loss function(trian, validation) =0.4351,0.3854\n",
        "*   accuracy(trian, validation) =0.9093,0.9095\n",
        "*   loss function(test) = 0.40.\n",
        "*   Test accuracy:0.9095\n",
        "\n",
        "other model\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 5\n",
        "*   activation function = relu for hidden layer \n",
        "*   activation function = relu for output layer \n",
        "*   excution time = 11s\n",
        "*   loss function(trian, validation) =2.2964, 2.2984\n",
        "*   accuracy(trian, validation) =0.1002,0.0990\n",
        "*   loss function(test) =2.30\n",
        "*   Test accuracy:9.92%\n",
        "\n",
        "other model\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 5\n",
        "*   activation function = tanh for hidden layer \n",
        "*   activation function = tanh for output layer \n",
        "*   excution time = 10s\n",
        "*   loss function(trian, validation) =2.1913 2.1900 \n",
        "*   accuracy(trian, validation) =0.2215 0.2220\n",
        "*   loss function(test) = 2.19.\n",
        "*   Test accuracy: 22.34%\n",
        "\n",
        "other model\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 5\n",
        "*   activation function = tanh for hidden layer \n",
        "*   activation function = softmax for output layer \n",
        "*   excution time = 10s\n",
        "*   loss function(trian, validation) = 0.0974 0.0930  \n",
        "*   accuracy(trian, validation) = 0.9708 0.9738\n",
        "*   loss function(test) = 0.12.\n",
        "*   Test accuracy:  96.36%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xuk1-nHmUBI",
        "colab_type": "text"
      },
      "source": [
        "# 5. Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmkjiIQWmT8H",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 4\n",
        "*   activation function = relu for 1,2 hidden layer tanh 3,4 layer \n",
        "*   activation function = soft max for output layer \n",
        "*   excution time = 11 s\n",
        "*   loss function(trian, validation) =0.0284 0.0363\n",
        "*   accuracy(trian, validation) =0.9913 0.9885\n",
        "*   loss function(test) =0.10\n",
        "*   accuracy(test) = 97.34%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoeZIVm6mT6n",
        "colab_type": "text"
      },
      "source": [
        "# 6. Adjust the batch size. Try a batch size of 10000. How does the required time change? What about the accuracy?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kilSwMZjmT01",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "*   batch size 10000\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 4\n",
        "*   activation function = relu for 1,2 hidden layer tanh 3,4 layer \n",
        "*   activation function = soft max for output layer \n",
        "*   excution time = 8s \n",
        "*   loss function(trian, validation) =0.1503 0.1436\n",
        "*   accuracy(trian, validation) = 0.9551  0.9583\n",
        "*   loss function(test) = 0.15\n",
        "*   accuracy(test) =95.44%\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEXJoIqmmTzX",
        "colab_type": "text"
      },
      "source": [
        "# 7. Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rKatkGm959",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "\n",
        "*   batch size 1\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 2\n",
        "*   activation function = relu for hidden layer \n",
        "*   activation function = soft max for output layer \n",
        "*   excution time = 84s \n",
        "*   loss function(trian, validation) =0.1251 0.1158\n",
        "*   accuracy(trian, validation) = 0.9672  0.9687\n",
        "*   loss function(test) = 0.12.\n",
        "*   accuracy(test) = 96.78%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjVNRsVJmTt9",
        "colab_type": "text"
      },
      "source": [
        "# 8. Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N10Fl5jZmTsc",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 4\n",
        "*   activation function = relu for1,2 hidden layer 3,4 tanh hidden layer \n",
        "*   activation function = soft max for output layer \n",
        "*   excution time = 11s \n",
        "*   loss function(trian, validation) =0.1574 0.1546 \n",
        "*   accuracy(trian, validation) = 0.9555  0.9552\n",
        "*   loss function(test) =0.16.\n",
        "*   accuracy(test) =95.25%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6sRqJ5-mTmV",
        "colab_type": "text"
      },
      "source": [
        "# 9. Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8xhK-BbmTk4",
        "colab_type": "text"
      },
      "source": [
        "Base model\n",
        "\n",
        "\n",
        "\n",
        "*   batch size 100\n",
        "*   hidden layer 50\n",
        "*   number of hidden layer = 4\n",
        "*   activation function = relu for1,2 hidden layer 3,4 tanh hidden layer \n",
        "*   activation function = soft max for output layer \n",
        "*   excution time = 11s \n",
        "*   loss function(trian, validation) =0.18574 0.15846 \n",
        "*   accuracy(trian, validation) = 0.8555  0.8552\n",
        "*   loss function(test) =0.196.\n",
        "*   accuracy(test) =80.25%"
      ]
    }
  ]
}