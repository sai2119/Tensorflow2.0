{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "first deep learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWEoxT5bxUqoEf0O8d49Mt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sai2119/Tensorflow2.0/blob/master/first_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hisi_whdT_ED",
        "colab_type": "text"
      },
      "source": [
        "Importing packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUWVIAGMTdSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNXa3LrgUS_u",
        "colab_type": "text"
      },
      "source": [
        "Initializing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9DRg-mGT9Pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obs=100000\n",
        "xs = np.random.uniform(low=-10, high=10, size=(obs,1))\n",
        "zs = np.random.uniform(-10, 10, (obs,1))\n",
        "generated_inputs = np.column_stack((xs,zs))\n",
        "noise = np.random.uniform(-1, 1, (obs,1))\n",
        "generated_targets = 2*xs - 3*zs + 5 + noise\n",
        "\n",
        "# save into an npz file called \"TF_intro\"\n",
        "np.savez('TF_intro', inputs=generated_inputs, targets=generated_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9DFBLi1VM-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.load('TF_intro.npz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKEYC7uBVgQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e1ac696-ce3f-4b5c-eec6-cf73425382b0"
      },
      "source": [
        "# Declare a variable where we will store the input size of our model\n",
        "# It should be equal to the number of variables you have\n",
        "input_size = 2\n",
        "# Declare the output size of the model\n",
        "# It should be equal to the number of outputs you've got (for regressions that's usually 1)\n",
        "output_size = 1\n",
        "\n",
        "# Outline the model\n",
        "# We lay out the model in 'Sequential'\n",
        "# Note that there are no calculations involved - we are just describing our network\n",
        "model = tf.keras.Sequential([\n",
        "                            # Each 'layer' is listed here\n",
        "                            # The method 'Dense' indicates, our mathematical operation to be (xw + b)\n",
        "                            tf.keras.layers.Dense(output_size,\n",
        "                                                 # there are extra arguments you can include to customize your model\n",
        "                                                 # in our case we are just trying to create a solution that is \n",
        "                                                 # as close as possible to our NumPy model\n",
        "                                                kernel_initializer=tf.random_uniform_initializer(minval=-0.01,maxval=0.1, seed=1525),\n",
        "                                                bias_initializer=tf.random_uniform_initializer(minval=-0.01,maxval=0.1, seed=1525)\n",
        "                                                 )\n",
        "                            ])\n",
        "\n",
        "# We can also define a custom optimizer, where we can specify the learning rate\n",
        "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "# Note that sometimes you may also need a custom loss function \n",
        "# That's much harder to implement and won't be covered in this course though\n",
        "\n",
        "# 'compile' is the place where you select and indicate the optimizers and the loss\n",
        "model.compile(optimizer=custom_optimizer, loss='mean_squared_error')\n",
        "\n",
        "# finally we fit the model, indicating the inputs and targets\n",
        "# if they are not otherwise specified the number of epochs will be 1 (a single epoch of training), \n",
        "# so the number of epochs is 'kind of' mandatory, too\n",
        "# we can play around with verbose; we prefer verbose=2\n",
        "model.fit(data['inputs'],data['targets'], epochs=100, verbose=2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3125/3125 - 3s - loss: 0.6795\n",
            "Epoch 2/100\n",
            "3125/3125 - 3s - loss: 0.3449\n",
            "Epoch 3/100\n",
            "3125/3125 - 3s - loss: 0.3455\n",
            "Epoch 4/100\n",
            "3125/3125 - 4s - loss: 0.3448\n",
            "Epoch 5/100\n",
            "3125/3125 - 4s - loss: 0.3445\n",
            "Epoch 6/100\n",
            "3125/3125 - 4s - loss: 0.3451\n",
            "Epoch 7/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 8/100\n",
            "3125/3125 - 3s - loss: 0.3444\n",
            "Epoch 9/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 10/100\n",
            "3125/3125 - 3s - loss: 0.3444\n",
            "Epoch 11/100\n",
            "3125/3125 - 3s - loss: 0.3450\n",
            "Epoch 12/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 13/100\n",
            "3125/3125 - 3s - loss: 0.3449\n",
            "Epoch 14/100\n",
            "3125/3125 - 3s - loss: 0.3454\n",
            "Epoch 15/100\n",
            "3125/3125 - 3s - loss: 0.3449\n",
            "Epoch 16/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 17/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 18/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 19/100\n",
            "3125/3125 - 3s - loss: 0.3446\n",
            "Epoch 20/100\n",
            "3125/3125 - 3s - loss: 0.3445\n",
            "Epoch 21/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 22/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 23/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 24/100\n",
            "3125/3125 - 3s - loss: 0.3450\n",
            "Epoch 25/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 26/100\n",
            "3125/3125 - 3s - loss: 0.3446\n",
            "Epoch 27/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 28/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 29/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 30/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 31/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 32/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 33/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 34/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 35/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 36/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 37/100\n",
            "3125/3125 - 3s - loss: 0.3450\n",
            "Epoch 38/100\n",
            "3125/3125 - 3s - loss: 0.3449\n",
            "Epoch 39/100\n",
            "3125/3125 - 3s - loss: 0.3454\n",
            "Epoch 40/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 41/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 42/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 43/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 44/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 45/100\n",
            "3125/3125 - 3s - loss: 0.3453\n",
            "Epoch 46/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 47/100\n",
            "3125/3125 - 3s - loss: 0.3445\n",
            "Epoch 48/100\n",
            "3125/3125 - 3s - loss: 0.3446\n",
            "Epoch 49/100\n",
            "3125/3125 - 3s - loss: 0.3449\n",
            "Epoch 50/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 51/100\n",
            "3125/3125 - 3s - loss: 0.3443\n",
            "Epoch 52/100\n",
            "3125/3125 - 3s - loss: 0.3455\n",
            "Epoch 53/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 54/100\n",
            "3125/3125 - 3s - loss: 0.3450\n",
            "Epoch 55/100\n",
            "3125/3125 - 3s - loss: 0.3449\n",
            "Epoch 56/100\n",
            "3125/3125 - 3s - loss: 0.3457\n",
            "Epoch 57/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 58/100\n",
            "3125/3125 - 3s - loss: 0.3453\n",
            "Epoch 59/100\n",
            "3125/3125 - 3s - loss: 0.3456\n",
            "Epoch 60/100\n",
            "3125/3125 - 3s - loss: 0.3449\n",
            "Epoch 61/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 62/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 63/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 64/100\n",
            "3125/3125 - 3s - loss: 0.3444\n",
            "Epoch 65/100\n",
            "3125/3125 - 3s - loss: 0.3443\n",
            "Epoch 66/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 67/100\n",
            "3125/3125 - 3s - loss: 0.3446\n",
            "Epoch 68/100\n",
            "3125/3125 - 3s - loss: 0.3454\n",
            "Epoch 69/100\n",
            "3125/3125 - 3s - loss: 0.3443\n",
            "Epoch 70/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 71/100\n",
            "3125/3125 - 3s - loss: 0.3450\n",
            "Epoch 72/100\n",
            "3125/3125 - 3s - loss: 0.3450\n",
            "Epoch 73/100\n",
            "3125/3125 - 3s - loss: 0.3455\n",
            "Epoch 74/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 75/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 76/100\n",
            "3125/3125 - 3s - loss: 0.3445\n",
            "Epoch 77/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 78/100\n",
            "3125/3125 - 3s - loss: 0.3449\n",
            "Epoch 79/100\n",
            "3125/3125 - 3s - loss: 0.3450\n",
            "Epoch 80/100\n",
            "3125/3125 - 3s - loss: 0.3455\n",
            "Epoch 81/100\n",
            "3125/3125 - 3s - loss: 0.3444\n",
            "Epoch 82/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 83/100\n",
            "3125/3125 - 3s - loss: 0.3447\n",
            "Epoch 84/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 85/100\n",
            "3125/3125 - 3s - loss: 0.3453\n",
            "Epoch 86/100\n",
            "3125/3125 - 3s - loss: 0.3445\n",
            "Epoch 87/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 88/100\n",
            "3125/3125 - 3s - loss: 0.3453\n",
            "Epoch 89/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 90/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 91/100\n",
            "3125/3125 - 3s - loss: 0.3451\n",
            "Epoch 92/100\n",
            "3125/3125 - 3s - loss: 0.3448\n",
            "Epoch 93/100\n",
            "3125/3125 - 3s - loss: 0.3449\n",
            "Epoch 94/100\n",
            "3125/3125 - 3s - loss: 0.3446\n",
            "Epoch 95/100\n",
            "3125/3125 - 3s - loss: 0.3452\n",
            "Epoch 96/100\n",
            "3125/3125 - 3s - loss: 0.3446\n",
            "Epoch 97/100\n",
            "3125/3125 - 3s - loss: 0.3444\n",
            "Epoch 98/100\n",
            "3125/3125 - 3s - loss: 0.3456\n",
            "Epoch 99/100\n",
            "3125/3125 - 3s - loss: 0.3444\n",
            "Epoch 100/100\n",
            "3125/3125 - 3s - loss: 0.3453\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1bf39d11d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I-wp2KFYlHO",
        "colab_type": "text"
      },
      "source": [
        "Extract the weights and bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWlabvMbYu4K",
        "colab_type": "text"
      },
      "source": [
        "Extracting the weight(s) and bias(es) of a model is not an essential step for the machine learning process. In fact, usually they would not tell us much in a deep learning context. However, this simple example was set up in a way, which allows us to verify if the answers we get are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYWK4TZGXhCI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "093e28b2-de30-4c1f-fe26-cb1e44bcde7a"
      },
      "source": [
        "# Extracting the weights and biases is achieved quite easily\n",
        "print(model.layers[0].get_weights())\n",
        "# We can save the weights and biases in separate variables for easier examination\n",
        "# Note that there can be hundreds or thousands of them!\n",
        "weights = model.layers[0].get_weights()[0]\n",
        "print(weights)\n",
        "# We can save the weights and biases in separate variables for easier examination\n",
        "# Note that there can be hundreds or thousands of them!\n",
        "bias = model.layers[0].get_weights()[1]\n",
        "print(bias)\n",
        "# We can predict new values in order to actually make use of the model\n",
        "# Sometimes it is useful to round the values to be able to read the output\n",
        "# Usually we use this method on NEW DATA, rather than our original training data\n",
        "print(model.predict_on_batch(data['inputs']).round(1))\n",
        "# If we display our targets (actual observed values), we can manually compare the outputs and the targets\n",
        "print(data['targets'].round(1))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[ 1.9917108],\n",
            "       [-2.9954576]], dtype=float32), array([5.007972], dtype=float32)]\n",
            "[[ 1.9917108]\n",
            " [-2.9954576]]\n",
            "[5.007972]\n",
            "[[ -0.6]\n",
            " [ 26.3]\n",
            " [  9. ]\n",
            " ...\n",
            " [-25. ]\n",
            " [ 24.1]\n",
            " [  5.6]]\n",
            "[[ -0.9]\n",
            " [ 25.7]\n",
            " [  8.4]\n",
            " ...\n",
            " [-24.7]\n",
            " [ 24.8]\n",
            " [  5.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM8iu7jzZKY1",
        "colab_type": "text"
      },
      "source": [
        "Plotting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_5JMXq4ZJfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "891ce85f-6936-4a3a-be8c-303a5a64cdd0"
      },
      "source": [
        "# The model is optimized, so the outputs are calculated based on the last form of the model\n",
        "\n",
        "# We have to np.squeeze the arrays in order to fit them to what the plot function expects.\n",
        "# Doesn't change anything as we cut dimensions of size 1 - just a technicality.\n",
        "plt.plot(np.squeeze(model.predict_on_batch(data['inputs'])), np.squeeze(data['targets']))\n",
        "plt.xlabel('outputs')\n",
        "plt.ylabel('targets')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9IuBoZ9cW22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}